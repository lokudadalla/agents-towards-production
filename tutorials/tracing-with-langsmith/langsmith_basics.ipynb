{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb594a94",
   "metadata": {},
   "source": [
    "![](https://europe-west1-atp-views-tracker.cloudfunctions.net/working-analytics?notebook=tutorials--tracing-with-langsmith--langsmith-basics)\n",
    "\n",
    "# LangSmith Tutorial: Adding Observability to AI Systems with LangGraph\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial demonstrates how to add observability to AI systems using LangSmith and LangGraph. While building AI applications has become more accessible, understanding how they make decisions and monitoring their behavior in real-world usage remains challenging.\n",
    "\n",
    "### Why Observability Matters\n",
    "\n",
    "Most AI applications work well in demonstrations but become difficult to debug and optimize when deployed. Without visibility into the decision-making process, teams struggle with fundamental questions: Why did the AI choose this particular response? Which parts of the system are slow or expensive? How can we systematically improve performance based on real usage patterns? What trajectories of tools are most effective? Which of these trajectories are the most effective, repetitive and cost effective?\n",
    "\n",
    "Think of observability as adding a \"flight recorder\" to your AI system. Just as airlines use black boxes to understand what happened during flights, LangSmith captures every decision, timing, and data flow in your AI system. This transforms AI development from guesswork into engineering.\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "By following this tutorial, you will understand how to instrument AI systems for transparency and observability. \n",
    "\n",
    "We'll build a simple research assistant using LangGraph that demonstrates key observability patterns. The assistant will analyze questions, decide whether to use tools, execute those tools when needed, and provide helpful responses. Throughout this process, LangSmith will capture detailed traces that show every decision point, timing data, and the flow of information through the system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336c3d7c",
   "metadata": {},
   "source": [
    "## Prerequisites and Initial Setup\n",
    "\n",
    "Before we begin building, we need to set up our development environment. This involves installing the necessary packages and configuring API keys for both OpenAI (which powers our AI) and LangSmith (which provides observability).\n",
    "\n",
    "Understanding this setup is crucial because LangSmith works by automatically intercepting and logging all LangGraph operations. Once configured, every LLM call, tool execution, and workflow step will be captured without requiring additional code changes. \n",
    "\n",
    "Requirements:\n",
    "- Python 3.9+ \n",
    "- OpenAI API key ([get one here](https://platform.openai.com/api-keys))\n",
    "- LangSmith account ([free signup](https://smith.langchain.com)) - This provides the observability dashboard where you'll see all the insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1496d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for AI development with observability\n",
    "!pip install -U langchain-core langchain-openai langgraph langsmith requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1817ca9",
   "metadata": {},
   "source": [
    "## API Configuration\n",
    "\n",
    "Now we'll configure the API keys and enable LangSmith tracing. The key insight here is that setting `LANGCHAIN_TRACING_V2=true` automatically enables comprehensive logging of all operations. Think of this as installing a \"flight recorder\" for your AI system. From this point forward, every decision and operation will be captured and made visible in your LangSmith dashboard.\n",
    "\n",
    "This is fundamentally different from traditional logging because LangSmith understands the structure of AI workflows. Instead of just capturing text logs, it builds a complete picture of how information flows through your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee65711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: OPENAI_API_KEY needs your actual key\n",
      "Warning: LANGCHAIN_API_KEY needs your actual key\n",
      "\n",
      "LangSmith Project: langsmith-tutorial-demo\n",
      "\n",
      "Tracing is now active - all AI operations will be logged for analysis\n",
      "Visit https://smith.langchain.com to see your traces\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configure API keys - replace with your actual keys\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "os.environ['LANGCHAIN_API_KEY'] = ''\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'  # This triggers observability\n",
    "os.environ['LANGCHAIN_PROJECT'] = 'langsmith-tutorial-demo' #This is the project name where the traces will be stored\n",
    "\n",
    "# Verify configuration\n",
    "required_vars = ['OPENAI_API_KEY', 'LANGCHAIN_API_KEY']\n",
    "for var in required_vars:\n",
    "    if not os.getenv(var) or 'your_' in os.getenv(var, ''):\n",
    "        print(f\"Warning: {var} needs your actual key\")\n",
    "    else:\n",
    "        print(f\"âœ“ {var} configured\")\n",
    "\n",
    "print(f\"\\nLangSmith Project: {os.getenv('LANGCHAIN_PROJECT')}\")\n",
    "print(\"\\nTracing is now active - all AI operations will be logged for analysis\")\n",
    "print(\"Visit https://smith.langchain.com to see your traces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdb8fea",
   "metadata": {},
   "source": [
    "## Building a Simple Observable Agent\n",
    "\n",
    "We'll create a minimal system that still demonstrates the power of LangSmith observability. Our agent will have just two capabilities: answering questions directly from its training data, and using a simple search tool when it needs current information.\n",
    "\n",
    "Let's start by setting up our basic components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bad6162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model initialized with temperature=0 for consistent behavior\n",
      "All LLM calls will be automatically traced in LangSmith\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.tools import tool\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Initialize the language model with deterministic settings\n",
    "# Using temperature=0 ensures consistent responses, making it easier to analyze patterns\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "print(\"Language model initialized with temperature=0 for consistent behavior\")\n",
    "print(\"All LLM calls will be automatically traced in LangSmith\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0540fbc3",
   "metadata": {},
   "source": [
    "## Defining Our Simple Agent State\n",
    "\n",
    "The agent's state represents the information that flows through our workflow. For our simple agent, we need just enough state to demonstrate meaningful observability. Think of this as the \"memory\" that LangSmith will track as it moves through each step of the process.\n",
    "\n",
    "Each field in this state serves a specific purpose for observability. The user_question field lets us correlate behavior with input types. The needs_search field shows us the agent's decision-making. The search_result field captures tool execution results. And the reasoning field provides explicit explanations that help us understand why the agent made specific choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b936d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state schema defined\n",
      "This structured state enables LangSmith to track information flow\n"
     ]
    }
   ],
   "source": [
    "class AgentState(TypedDict):\n",
    "    \"\"\"Simple state that flows through our agent workflow.\"\"\"\n",
    "    user_question: str        # The original question from the user\n",
    "    needs_search: bool        # Whether we determined search is needed\n",
    "    search_result: str        # Result from our search tool (if used)\n",
    "    final_answer: str         # The response we'll give to the user\n",
    "    reasoning: str            # Why we made our decisions (great for observability)\n",
    "\n",
    "print(\"Agent state schema defined\")\n",
    "print(\"This structured state enables LangSmith to track information flow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f17f0d",
   "metadata": {},
   "source": [
    "## Creating Our Search Tool\n",
    "\n",
    "We'll implement a search function that can fetch current information from Wikipedia. This tool demonstrates how LangSmith captures tool execution details, including timing, success/failure status, and returned data.\n",
    "\n",
    "The implementation uses Wikipedia's search API properly. Unlike a page summary endpoint that requires exact page titles, the search API can handle general queries and return relevant results. Notice how we include comprehensive error handling and logging statements. These will appear in your LangSmith traces, helping you understand what happens during execution.\n",
    "\n",
    "Understanding the difference between summary and search APIs is important: summary APIs expect exact page titles (like \"Artificial_intelligence\") while search APIs can handle natural language queries (like \"what is AI\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ba62bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search tool created with proper Wikipedia search API integration\n",
      "Tool execution timing and results will be captured automatically\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def wikipedia_search(query: str) -> str:\n",
    "    \"\"\"Search Wikipedia for current information about a topic.\"\"\"\n",
    "    try:\n",
    "        # Use Wikipedia's proper search API that can handle general queries\n",
    "        # This is different from the page summary API which requires exact page titles\n",
    "        search_url = \"https://en.wikipedia.org/w/api.php\"\n",
    "        search_params = {\n",
    "            \"action\": \"query\",\n",
    "            \"list\": \"search\",\n",
    "            \"srsearch\": query,\n",
    "            \"format\": \"json\",\n",
    "            \"srlimit\": 3  # Get top 3 results\n",
    "        }\n",
    "        \n",
    "        response = requests.get(search_url, params=search_params, timeout=10)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            search_results = data.get('query', {}).get('search', [])\n",
    "            \n",
    "            if search_results:\n",
    "                # Get the most relevant result and fetch its summary\n",
    "                top_result = search_results[0]\n",
    "                page_title = top_result['title']\n",
    "                \n",
    "                # Now get the page summary using the exact title\n",
    "                summary_url = f\"https://en.wikipedia.org/api/rest_v1/page/summary/{page_title.replace(' ', '_')}\"\n",
    "                summary_response = requests.get(summary_url, timeout=10)\n",
    "                \n",
    "                if summary_response.status_code == 200:\n",
    "                    summary_data = summary_response.json()\n",
    "                    extract = summary_data.get('extract', 'No summary available')\n",
    "                    # Truncate for readability in traces\n",
    "                    return f\"Found information about '{page_title}': {extract[:400]}...\"\n",
    "                else:\n",
    "                    return f\"Found '{page_title}' but couldn't retrieve summary\"\n",
    "            else:\n",
    "                return f\"No Wikipedia articles found for '{query}'\"\n",
    "        else:\n",
    "            return f\"Wikipedia search failed with status {response.status_code}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        # This error handling will show up in LangSmith traces\n",
    "        return f\"Search error: {str(e)}\"\n",
    "\n",
    "print(\"Search tool created with proper Wikipedia search API integration\")\n",
    "print(\"Tool execution timing and results will be captured automatically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a8d3ee",
   "metadata": {},
   "source": [
    "## Building the Decision-Making Workflow\n",
    "\n",
    "Now we'll create the core logic of our agent. Even though this is a simple system, we'll structure it as separate functions to demonstrate how LangSmith traces multi-step workflows. Each function represents a clear decision point that will be visible in your observability dashboard.\n",
    "\n",
    "This modular approach serves two purposes: it makes the code easier to understand and test, and it creates natural breakpoints that LangSmith can capture and analyze. \n",
    "\n",
    "### Step 1: Deciding Whether to Search\n",
    "\n",
    "The first step analyzes the user's question and decides whether we need to search for current information, or if we can answer directly from the model's training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "472f5ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_search_need(state: AgentState) -> AgentState:\n",
    "    \"\"\"Analyze the question and decide if we need to search for current information.\"\"\"\n",
    "    user_question = state[\"user_question\"]\n",
    "    \n",
    "    # This prompt engineering is visible in LangSmith traces\n",
    "    # Notice how we're asking for a structured response to make parsing reliable\n",
    "    decision_prompt = f\"\"\"\n",
    "    Analyze this question and decide if it requires current/recent information that might not be in your training data:\n",
    "    \n",
    "    Question: \"{user_question}\"\n",
    "    \n",
    "    Consider:\n",
    "    - Does this ask about recent events, current prices, or breaking news?\n",
    "    - Does this ask about people, companies, or topics that change frequently?\n",
    "    - Can you answer this well using your existing knowledge?\n",
    "    \n",
    "    Respond with exactly \"SEARCH\" if you need current information, or \"DIRECT\" if you can answer directly.\n",
    "    Then on a new line, briefly explain your reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke([SystemMessage(content=decision_prompt)])\n",
    "    decision_text = response.content.strip()\n",
    "    \n",
    "    # Parse the response - this parsing logic will be visible in traces\n",
    "    lines = decision_text.split('\\n')\n",
    "    decision = lines[0].strip()\n",
    "    reasoning = lines[1] if len(lines) > 1 else \"No reasoning provided\"\n",
    "    \n",
    "    # Update state with our decision\n",
    "    state[\"needs_search\"] = decision == \"SEARCH\"\n",
    "    state[\"reasoning\"] = f\"Decision: {decision}. Reasoning: {reasoning}\"\n",
    "    \n",
    "    # This print statement will help you see the flow during execution\n",
    "    print(f\"Decision: {'SEARCH' if state['needs_search'] else 'DIRECT'} - {reasoning}\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88989d07",
   "metadata": {},
   "source": [
    "### Step 2: Executing Search When Needed\n",
    "\n",
    "If the previous step determined that search is needed, this function executes our search tool. LangSmith shows not just whether the search happened, but exactly what query was sent, how long it took, and what results came back.\n",
    "\n",
    "This conditional execution pattern is common in AI systems, and LangSmith handles it by showing you which path was taken and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d71c22df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_search(state: AgentState) -> AgentState:\n",
    "    \"\"\"Execute search if needed, otherwise skip this step.\"\"\"\n",
    "    if not state[\"needs_search\"]:\n",
    "        print(\"Skipping search - not needed for this question\")\n",
    "        state[\"search_result\"] = \"No search performed\"\n",
    "        return state\n",
    "    \n",
    "    print(f\"Executing search for: {state['user_question']}\")\n",
    "    \n",
    "    # Execute our search tool - this will show up as a separate step in LangSmith\n",
    "    # The .invoke() call will be traced with full input/output details\n",
    "    search_result = wikipedia_search.invoke({\"query\": state[\"user_question\"]})\n",
    "    state[\"search_result\"] = search_result\n",
    "    \n",
    "    print(f\"Search completed: {len(search_result)} characters returned\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dbeb26",
   "metadata": {},
   "source": [
    "### Step 3: Generating the Final Response\n",
    "\n",
    "The final step synthesizes all available information into a helpful response. This is where we combine the model's built-in knowledge with any search results we gathered. The synthesis process is often the most complex part of an AI system, and LangSmith's observability helps you understand how well this process works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6922d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(state: AgentState) -> AgentState:\n",
    "    \"\"\"Generate the final response using all available information.\"\"\"\n",
    "    user_question = state[\"user_question\"]\n",
    "    search_result = state.get(\"search_result\", \"\")\n",
    "    used_search = state[\"needs_search\"]\n",
    "    \n",
    "    # Build context for the response\n",
    "    # This conditional logic creates different prompt patterns that LangSmith will capture\n",
    "    if used_search and \"Search error\" not in search_result:\n",
    "        context = f\"Question: {user_question}\\n\\nSearch Results: {search_result}\"\n",
    "        response_prompt = f\"\"\"\n",
    "        Answer the user's question using both your knowledge and the search results provided.\n",
    "        \n",
    "        {context}\n",
    "        \n",
    "        Provide a helpful, accurate response that synthesizes the information.\n",
    "        \"\"\"\n",
    "    else:\n",
    "        response_prompt = f\"\"\"\n",
    "        Answer this question using your existing knowledge:\n",
    "        \n",
    "        {user_question}\n",
    "        \n",
    "        Provide a helpful, accurate response.\n",
    "        \"\"\"\n",
    "    \n",
    "    # This LLM call will be traced with the complete prompt and response\n",
    "    response = llm.invoke([SystemMessage(content=response_prompt)])\n",
    "    state[\"final_answer\"] = response.content\n",
    "    \n",
    "    print(f\"Response generated: {len(response.content)} characters\")\n",
    "    \n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5f35cf",
   "metadata": {},
   "source": [
    "## Assembling the Workflow\n",
    "\n",
    "Now we'll connect our functions into a complete workflow using LangGraph. This creates an explicit graph structure that LangSmith can visualize, showing you the exact path your AI takes through the decision-making process.\n",
    "\n",
    "The graph structure is particularly valuable for observability because it makes the control flow explicit. Instead of having conditional logic buried in function calls, LangGraph creates a visual representation that shows exactly which steps were executed and in what order.\n",
    "\n",
    "Think of this as creating a roadmap that LangSmith can follow to show you the journey your data took through the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89b0a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workflow compiled successfully\n",
      "Flow: decide â†’ search â†’ generate_response\n",
      "Ready to demonstrate LangSmith observability\n"
     ]
    }
   ],
   "source": [
    "# Build the workflow graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Add our three workflow steps\n",
    "# Each of these will appear as a distinct node in LangSmith's graph visualization\n",
    "workflow.add_node(\"decide\", decide_search_need)\n",
    "workflow.add_node(\"search\", execute_search)\n",
    "workflow.add_node(\"respond\", generate_response)\n",
    "\n",
    "# Define the flow with conditional logic\n",
    "# LangSmith will show you which edges were traversed for each execution\n",
    "workflow.set_entry_point(\"decide\")\n",
    "workflow.add_edge(\"decide\", \"search\")     # Always go to search step (it will skip if not needed)\n",
    "workflow.add_edge(\"search\", \"respond\")    # Then generate response\n",
    "workflow.add_edge(\"respond\", END)         # Finish\n",
    "\n",
    "# Compile into an executable agent\n",
    "simple_agent = workflow.compile()\n",
    "\n",
    "print(\"Workflow compiled successfully\")\n",
    "print(\"Flow: decide â†’ search â†’ generate_response\")\n",
    "print(\"Ready to demonstrate LangSmith observability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26741b13",
   "metadata": {},
   "source": [
    "## The Workflow Visualization\n",
    "\n",
    " Below is a visualization of our agent's workflow graph. This diagram shows the structure that LangSmith will trace during execution:\n",
    " \n",
    "#  <img src=\"assets/wiki_agent_td.png\" alt=\"Agent Workflow Graph\" width=\"700\">\n",
    " \n",
    " This visualization helps us understand the flow between the different components of our agent: decision-making, search execution, and response generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edd090f",
   "metadata": {},
   "source": [
    "## Testing with Full Observability\n",
    "\n",
    "We'll run our agent on different types of questions and see how LangSmith captures every detail of the execution. Each test will generate a complete trace that shows you the decision-making process, timing information, and all intermediate results.\n",
    "\n",
    "This test runner includes timing measurements and metadata tagging, which helps organize your traces in LangSmith for easier analysis. The metadata and tags serve as filters that let you group and compare similar types of executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "357becf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_with_observability(question: str, test_type: str) -> dict:\n",
    "    \"\"\"Run a test and capture comprehensive observability data.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {question}\")\n",
    "    print(f\"Type: {test_type}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize state for this test\n",
    "    initial_state = {\n",
    "        \"user_question\": question,\n",
    "        \"needs_search\": False,\n",
    "        \"search_result\": \"\",\n",
    "        \"final_answer\": \"\",\n",
    "        \"reasoning\": \"\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Execute with metadata for LangSmith\n",
    "        # This metadata will help you filter and analyze traces later\n",
    "        config = {\n",
    "            \"metadata\": {\n",
    "                \"test_type\": test_type,\n",
    "                \"tutorial\": \"langsmith-observability\"\n",
    "            },\n",
    "            \"tags\": [\"tutorial\", \"demo\", test_type]\n",
    "        }\n",
    "        \n",
    "        # This invoke call will create a complete trace in LangSmith\n",
    "        final_state = simple_agent.invoke(initial_state, config=config)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        # Display results for immediate feedback\n",
    "        print(f\"\\nResults:\")\n",
    "        print(f\"   Decision Process: {final_state['reasoning']}\")\n",
    "        print(f\"   Used Search: {'Yes' if final_state['needs_search'] else 'No'}\")\n",
    "        print(f\"   Response Length: {len(final_state['final_answer'])} characters\")\n",
    "        print(f\"   Total Time: {total_time:.2f} seconds\")\n",
    "        print(f\"\\nAnswer: {final_state['final_answer'][:200]}...\")\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"type\": test_type,\n",
    "            \"success\": True,\n",
    "            \"used_search\": final_state['needs_search'],\n",
    "            \"total_time\": round(total_time, 2),\n",
    "            \"reasoning\": final_state['reasoning']\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"type\": test_type,\n",
    "            \"success\": False,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd9098f",
   "metadata": {},
   "source": [
    "## Running Our Test Suite\n",
    "\n",
    "Let's test our agent with questions that should trigger different behaviors. We have three types of questions that will help us understand how the agent makes decisions:\n",
    "\n",
    "**Direct Answer**: Questions the model can answer from training data. These should show the agent choosing not to search, demonstrating efficient resource usage.\n",
    "\n",
    "**Current Info**: Questions requiring recent information. These should trigger search behavior, showing how the agent handles information that changes over time.\n",
    "\n",
    "**Factual Lookup**: Questions about established facts that might benefit from verification. These will show how the agent balances confidence in its training data against the value of current verification.\n",
    "\n",
    "Watch how each question type flows through the system differently, and pay attention to the decision-making process that LangSmith captures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc35179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"question\": \"What is the capital of France?\",\n",
    "        \"type\": \"direct_answer\",\n",
    "        \"expected_search\": False\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What happened in the 2024 US presidential election?\",\n",
    "        \"type\": \"current_info\",\n",
    "        \"expected_search\": True\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Tell me about artificial intelligence\",\n",
    "        \"type\": \"factual_lookup\",\n",
    "        \"expected_search\": False  # Should be answerable directly\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"Starting LangSmith Observability Demo\")\n",
    "print(\"Each test will generate detailed traces in your LangSmith dashboard\")\n",
    "print(\"Visit https://smith.langchain.com to see real-time traces\\n\")\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for i, test_case in enumerate(test_cases, 1):\n",
    "    print(f\"\\nRunning Test {i}/{len(test_cases)}\")\n",
    "    \n",
    "    result = run_test_with_observability(\n",
    "        test_case[\"question\"], \n",
    "        test_case[\"type\"]\n",
    "    )\n",
    "    \n",
    "    test_results.append(result)\n",
    "    \n",
    "    # Small delay to make the traces easier to distinguish in LangSmith\n",
    "    time.sleep(1)\n",
    "\n",
    "print(f\"\\n\\nAll tests completed\")\n",
    "print(f\"Generated {len(test_results)} traces in LangSmith\")\n",
    "print(f\"Check your dashboard to explore the detailed execution data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff384047",
   "metadata": {},
   "source": [
    "## Understanding Your LangSmith Dashboard\n",
    "\n",
    "Now that you've run the tests, let's explore what LangSmith has captured. Navigate to your LangSmith dashboard at [smith.langchain.com](https://smith.langchain.com) and select the `langsmith-tutorial-demo` project.\n",
    "\n",
    "### What You'll See in LangSmith\n",
    "\n",
    "**Trace List View**: You'll see a list of all your test executions. Each row represents one complete run of your agent, showing the input question, execution time, success/failure status, and total cost. This view gives you a high-level overview of system performance across different types of queries.\n",
    "\n",
    "![Local image](./assets/1.png)\n",
    "\n",
    "Think of this as your system's activity log, but with much richer information than traditional logs. You can sort by execution time to find slow queries, filter by tags to analyze specific types of questions, or look for patterns in cost per query.\n",
    "\n",
    "**Individual Trace Details**: Click on any trace to see the complete execution flow. You'll see:\n",
    "\n",
    "![Local image](./assets/2.png)\n",
    "**Graph Visualization**: A visual representation of your workflow showing which nodes were executed and how data flowed between them. This is particularly powerful because you can see at a glance whether search was used and how long each step took.\n",
    "\n",
    "**Step-by-step Execution**: Each function call with inputs, outputs, and timing. This granular view helps you understand not just what happened, but why it happened. You can see the exact prompts sent to the language model and the reasoning it provided.\n",
    "\n",
    "![Local image](./assets/3.png)\n",
    "\n",
    "**LLM Calls**: Every prompt sent to the language model with the exact response. This transparency is crucial for prompt optimization and understanding model behavior.\n",
    "\n",
    "**Tool Executions**: When and how your search tool was called, including the query sent, response received, and execution time.\n",
    "\n",
    "**Performance Analytics**: LangSmith automatically aggregates performance data across all your runs:\n",
    "\n",
    "**Latency Patterns**: Which steps consistently take the longest? Is search always the bottleneck, or does decision-making sometimes slow things down?\n",
    "\n",
    "**Cost Analysis**: How much does each type of query cost? Are search queries significantly more expensive than direct answers?\n",
    "\n",
    "**Success Rates**: Are there categories of queries that consistently fail? Do certain question patterns lead to search errors?\n",
    "\n",
    "**Tool Usage Patterns**: How often is search actually needed? Are there questions that trigger search unnecessarily?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b4540",
   "metadata": {},
   "source": [
    "## Key Observability Insights\n",
    "\n",
    "Based on the traces you've just generated, here are the types of insights LangSmith enables. These insights transform how you understand and improve AI systems.\n",
    "\n",
    "### Decision-Making Transparency\n",
    "\n",
    "For each question, you can see exactly why the agent chose to search or answer directly. This transparency is crucial for debugging unexpected behavior and building trust in AI systems. When users ask why the system behaved a certain way, you can point to specific reasoning captured in the traces.\n",
    "\n",
    "### Performance Optimization Opportunities\n",
    "\n",
    "By comparing execution times across different question types, you can identify bottlenecks and optimization opportunities. For example, you might discover that search queries take significantly longer than direct answers, suggesting opportunities for caching or parallel execution.\n",
    "\n",
    "### Cost Management\n",
    "\n",
    "LangSmith shows you the token usage and estimated cost for each LLM call. This granular cost data helps you optimize in ways that would be impossible without observability. You can identify expensive operations and optimize prompts or routing logic to reduce costs without sacrificing quality.\n",
    "\n",
    "### Quality Assurance\n",
    "\n",
    "With complete traces, you can verify that the agent is making reasonable decisions consistently. You can spot patterns in successful versus failed executions, identify edge cases that need additional handling, and create regression tests based on real usage patterns.\n",
    "\n",
    "This systematic approach to quality assurance is only possible with comprehensive observability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9197a5e",
   "metadata": {},
   "source": [
    "## Taking This Further\n",
    "\n",
    "This tutorial demonstrated the fundamental principles of AI observability using a simple agent. The patterns you've learned scale to much more complex systems. Here's are some tips to get started:\n",
    "\n",
    "### For Your Own Projects\n",
    "\n",
    "**Start with observability from day one**: Enable LangSmith tracing before building complex features. \n",
    "\n",
    "**Structure your workflows**: Use LangGraph's explicit workflow structure to make decision points visible. \n",
    "\n",
    "**Add meaningful metadata**: Tag your traces with business context to enable better analysis.\n",
    "\n",
    "**Monitor key metrics**: Set up alerts for latency, cost, and error rates based on the patterns you observe in LangSmith.\n",
    "\n",
    "\n",
    "The observability foundation you've built here becomes even more valuable in production environments where understanding system behavior is critical for maintaining service quality and user trust.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901b069",
   "metadata": {},
   "source": [
    "---\n",
    "This tutorial was written by [Shivnarayan Rajappa](https://www.linkedin.com/in/shivnarayanrajappa/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
